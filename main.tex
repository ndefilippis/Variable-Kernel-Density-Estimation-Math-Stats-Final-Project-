\documentclass{article}
\usepackage{graphicx} % Required for inserting images

\usepackage{amsmath}
\usepackage{geometry}[margin=1in]
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{biblatex}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{xurl}
\usepackage{subcaption}
\addbibresource{ref.bib}

\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\var}[1]{\text{Var}\left(#1\right)}
\newcommand{\dd}[1]{\,\text{d}#1}

\title{Mathematical Statistics Final Project: Variable Kernel Density Estimation}
\author{Nick DeFilippis and Paco Rilloraza}
\date{12 December 2024}

\begin{document}

\maketitle

The fundamental question of statistics is the following: given observations $X_1, \dots, X_n$, from what probability measure did they arise? Often our candidate distributions arise from a family of measures $\mathcal{P}$ that can be parameterized by a parameter set $\Theta \subseteq \R^k$, i.e., for each $\theta \in \Theta$, there is a measure $\mathbb{P}_\theta \in \mathcal{P}$ \cite{jnw}. Assuming that $\theta \neq \theta'$ implies that $\mathbb{P}_\theta \neq \mathbb{P}_{\theta'}$, the question of finding the measure that explains our observations is equivalent to finding the optimal parameter. 

However, there are many cases in which $\mathcal{P}$ cannot be naturally parametrized. In this case, to find the explanatory $\mathbb{P} \in \mathcal{P}$, we must resort to the methods of nonparametric statistics. One such method is \textit{kernel density estimation} (KDE). KDE is a method for constructing an estimator $\hat{f}$ of a sufficiently smooth probability density $f$ from data. Such an estimator can be written as a sum of kernel functions centered at the data. Each of these kernel functions is a bump, and in most applications, the widths of these bumps are equal. In this setting, the figure of merit of the estimator is the \textit{mean-integrated square error} (MISE). In \cite{vkde}, Terrell and Scott study two proposals for KDEs with variable width, contrasting the MISEs of these variable KDEs with results from the fixed-width setting at varying numbers of samples $n$ and dimension $d$. 

In the univariate case, the variable-width estimators do not significantly improve the fixed-width estimators in most cases. However, they show in numerical experiments that in the setting with a small number of samples, some variable-width estimators can be appreciably better than the fixed-width case. We then transition to the multivariate case, where the picture is much more optimistic. As the number of dimensions increases, the variable-width estimators have a smaller MISE than the standard fixed density estimate.

We conclude with a numerical experiment comparing a fixed-width kernel density estimator with the two estimators that Terrell and Scott proposed. Our experiment is consistent with their results that variable-width kernel density estimators are only an improvement over their fixed-width counterparts in high-dimensional settings.

\section{Introduction and generalized kernels}
% 1. Introduction. Two types of VKDEs: h is a f'n of y or x. First promising for high dimensions, second for moderate sample sizes, but both bad

The standard form of a nonparametric kernel density estimator with a fixed bandwidth is
\begin{equation}\label{eq:fixed-width-kde}
    \hat{f}(\mathbf{y}) = \frac{1}{nh^d}\sum_{i=1}^nK\left(\frac{\mathbf{X}_i-\mathbf{y}}{h}\right)
\end{equation}

where $\mathbf{X}_i$ is an i.i.d random variable drawn from $\mathbf{P}_\theta$, and $K \in L^1(\R^d) \cap L^2(\R^d)$ is an order-$p$ kernel function, i.e., a unit mass function $K$ with $\lim_{z \to \infty} \abs{zK(z)} = 0$ and first nonzero moment being the $p$th moment, which is $\pm 1$ \cite{vkde}. Terrell and Scott introduce the idea of varying the size of the bandwidth. They focus on two perspectives of density estimators. We can specify how each observation $\mathbf{x}_i$ influences a given point $\mathbf{y}$. This perspective leads to balloon estimators. On the other hand, we can view the estimate as a combination of densities for each observation, which is the basis of sample smoothing estimators.

% 2. Generalized kernels. Multivariate density estimators that are continuous and Gateaux (directionally) differentiable can be written as an average of the Gateaux derivatives. Asymptotically, these become generalized KDEs. KDEs are (1) weights centered at y that specify how x_i influences estimate at y and (2) mixtures of densities centered at x_i. VKDEs generalize these.
Noting that fixed-width KDEs receive the difference $\mathbf{X}_i-\mathbf{y}$ as an argument to $K$, the most broad generalization is an estimator of the form 
\begin{equation}\label{eq:gen-kde}
    \hat{f}(\mathbf{y}) = \frac{1}{n} \sum_{i=1}^n K_n(\mathbf{X}_i,\mathbf{y}) 
\end{equation}
for a suitable function $K_n$. To justify this form, the authors provide Theorem 1, which states that for any multivariate density estimator $\hat{f}$ that is sufficiently smooth, we can write 
\begin{equation}\label{eq:thm1}
    \hat{f}(\mathbf{y}) = \frac{1}{n} \sum_{i=1}^n K(\mathbf{X}_i,\mathbf{y},\hat{F}_n)
\end{equation}
where $K(\mathbf{X}_i,\mathbf{y},\hat{F}_n)$ is a particular directional derivative\footnote{Actually, $K$ has a term that is a G\^ateaux derivative of the operator that takes an empirical CDF to an empirical density. A G\^ateaux derivative is a generalization of a direction derivative for certain topological vector spaces \cite{gateaux}.} of $\hat{f}$ that depends on the empirical cumulative distribution function (CDF) $\hat{F}_n$, i.e., it depends on all of the data $(\mathbf{X}_i)_{i=1}^n$. However, for large $n$, the Glivenko-Cantelli theorem \cite{jnw} says that $\hat{F}_n$ uniformly converges to the true CDF $F$, so in this limit, $K\mathbf{X}_i,\mathbf{y},\hat{F}_n)$ depends on $F$ and is thus independent of all of the data except for $\mathbf{X}_i$. Thus, equation \eqref{eq:thm1} can asymptotically be written as equation \eqref{eq:gen-kde}. 

\section{Univariate balloon estimators}
% 3. Balloon estimators have no AMISE improvement
Restricting to the one-dimensional setting, a univariate balloon estimator is a variable KDE of the form 
\begin{equation}\label{eq:balloon1}
    \hat{f}(y) = \frac{1}{nh(y)} \sum_{k=1}^n K\left(\frac{X_i-y}{h(y)}\right), 
\end{equation}
where $K$ is an order-$p$ univariate kernel. It is similar in form to a fixed-width KDE, except that the width $h$ depends on the observation point $y$. As in the fixed-width case, we have the asymptotic behavior
\begin{align}
    \var{\hat{f}(y)} &= O\left(\frac{1}{nh(y)}\right), \label{eq:ub-var} \\
    \left( \E \hat{f}(y) - f(y) \right)^2 &= O\left(h^{2p}\right). \label{eq:ub-bias}
\end{align}
Recalling that the MISE is the integral of the sum of these, simple calculus shows that asymptotically the optimal bandwidth is $h \asymp n^{-1/(2p+1)}$, in which case the MISE is $O\left(n^{-2p/(2p+1)}\right)$. Fixed-width and balloon estimators thus have asymptotic MISEs that differ only by a constant factor which, for reference, is 
\begin{equation}\label{eq:ub-compare}
    \frac{\text{balloon asymptotic MISE}}{\text{fixed-width asymptotic MISE}} = \dfrac{\displaystyle\int f^{2p/(2p+1)} (f^{(p)})^{2/(2p+1)}}{\left[ \displaystyle\int (f^{(p)})^2 \right]^{1/(2p+1)}} \leq 1. 
\end{equation}
 However, a numerical experiment with Gaussian and Cauchy densities estimated with kernels at several orders demonstrates that this factor is commonly around 0.7 to 0.95. Thus in the univariate case, balloon estimators do not drastically improve the convergence rate of the MISE. To emphasize this point, the authors discuss a commonly used balloon estimator by Loftsgaarden and Quesenberry with $h(y)$ being the Euclidean distance between $y$ and the $k$th nearest datum. They show that the bias term contributes infinitely to the asymptotic MISE, yielding an improvement factor of zero, unless certain restrictive conditions on the density $f$ are met. For these reasons, the authors aptly conclude that in the univariate setting, balloon estimators are ``mostly hot air.''

\section{Sample smoothing estimators}
% 4. Sample smoothing kernels have improved MISE for few samples, but no O(n^-8/9). Why? In the Gaussian data and uniform kernel case, the bias integral has some tail contributions yet unaccounted. There are different optimal bandwidths for n < 500 and n -> \infty. However, a monotonicity condition in this result complicates analysis
Remaining in the one-dimensional setting, a sample smoothing estimator is a variable KDE of the form 
\begin{equation}\label{eq:ss1}
    \hat{f}(y) = \frac{1}{n} \sum_{k=1}^n \frac{1}{h(X_i)}K\left(\frac{X_i-y}{h(X_i)}\right), 
\end{equation}
with $K$ again an order-$p$ univariate kernel. Here, $h$ depends on the datum $X_i$. The authors show that while the asymptotic variance has the same form as equation \eqref{eq:ub-var}, the bias is 
\begin{equation}\label{eq:ss-bias}
    \left(\E \hat{f}(y) - f(y)\right)^2 \sim \left( \frac{1}{p!} \frac{\text{d}^p}{\text{d}y^p} \left(h(y)^pf(y)\right) \right)^2, 
\end{equation}
a more complicated dependence on $h$. Abramson's estimator is a sample smoothing estimator where 
\begin{equation}\label{eq:abr}
    h(X_i) = hf(X_i)^{-1/2}
\end{equation}
for some $h>0$. This choice kills the asymptotic bias \eqref{eq:ss-bias} when $p=2$. 

\sloppy To investigate the mean-squared error (MSE), the authors analyze a special case where $X_1, \dots, X_n \sim \mathcal{N}(0,1)$ and $K$ the uniform kernel on $[-1,1]$, focusing on the MSE evaluated at $y=0$, which we abbreviate by MSE(0). In this case, the Abramson estimator has mean 
\begin{equation}\label{eq:ss-ex-mean}
    \E \hat{f}(0) = \int \frac{\phi(x)}{h} K\left(\frac{x\sqrt{\phi(x)}}{h}\right) \phi(x)\dd{x}
\end{equation}
at $y=0$. Here, $\phi$ is the standard normal density. Because $K$ is supported on $[-1,1]$, the domain of integration can be just 
\begin{equation}\label{eq:ss-ex-dom}
    \left\{x \in \R : \abs{\frac{x\sqrt{\phi(x)}}{h}} \leq 1\right\}.
\end{equation}
For sufficiently large $h$, however, this set is equal to $\R$. For other $h$, this set is of the form $(-\infty,-b) \cup (-a,a) \cup(b,\infty)$. Prior analysis only considered the contribution of $(-a, a)$ to the integral. This eventually leads to a squared bias term that is $O\left(h^4\right)$ and an optimal MSE(0) convergence rate that is $O\left(n^{-8/9}\right)$. However, there are significant tail contributions to \eqref{eq:ss-ex-mean} which are detailed in \cite{vkde}. These lead to a squared bias term that is actually $O\left(\left(h / \log h\right)^4\right)$, which converges to zero faster as $h \to 0$ and leads to an optimal MSE(0) convergence rate that is asymptotically $O\left(n^{-4/5}\right)$. The authors also consider the non-asymptotic regime, i.e., the regime where the number of samples is less than 500. In this case, it is actually possible to find an $h$ that kills the squared bias term. Because of this, the authors put forth two optimal values of $h$ for the asymptotic and non-asymptotic regimes and suggest, by means of a numerical experiment involving $n=10^3$ samples, that the non-asymptotic $h$ performs qualitatively better.

Regarding the MISE of the Abramson estimator, rather than just the MSE(0), Abramson's choice to have a fixed $h$ in their definition of $h(X_i)$ is suboptimal for general observation points $y \in \R$. Consequently, the MISE of the Abramson estimator actually converges more slowly than that of a fixed-width estimator for large numbers of samples. However, it does perform noticeably better for small numbers of samples; in one experiment with Gaussian data and $K(x) \propto (1-x^2)_+^2$, the authors report a critical number of samples $n=3 \times 10^4$ and opine that the improvement is appreciable only for $n<10^3$

\section{Multivariate balloon estimators}
% 5. Multivariate balloon estimators can perform better at high dimensions

Kernel density estimation can be generalized to the multivariate setting by defining a bandwidth matrix. The multivariate fixed kernel estimate is
\begin{equation}\label{eq:fixed-width-multid}
    \hat{f}(\mathbf{y}) = \frac{1}{n\vert H\vert}\sum_{i=1}^n K\left(H^{-1}(\mathbf{x}_i - \mathbf{y})\right)
\end{equation}
so multivariate balloon estimation follows by letting the bandwidth matrix vary with $\mathbf{y}$,
\begin{equation}\label{eq:balloon-multi}
    \hat{f}(\mathbf{y}) = \frac{1}{n\vert H(\mathbf{y})\vert}\sum_{i=1}^n K\left(H(\mathbf{y})^{-1}(\mathbf{x}_i - \mathbf{y})\right).
\end{equation}
Here $K : \R^d \to \R$ is an order-$p$ multivariate kernel which enjoys many similar properties as a univariate kernel, except that the moment conditions are 
\begin{align}
    \begin{aligned}
        \int z_1^{n_1} \cdots z_d^{n_d} K(\mathbf{z}) \dd{\mathbf{z}} &= 0& &\quad \text{for} \ 0 < \sum_{i=1}^d n_i < p, \\
        \int z_i^p K(\mathbf{z}) &= \pm 1& &\quad \text{for} \ i = 1, \dots, d.
    \end{aligned}
\end{align}
Additionally, for convenience, we often write $H(\mathbf{y}) = h(\mathbf{y})A(\mathbf{y})$, where $\abs{A(\mathbf{y}} = 1$ so that the width scale at $\mathbf{y}$ is determined entirely by $h(\mathbf{y})$.

Under sufficient regularity conditions on $f$, he $d$-dimensional analogues of equation \eqref{eq:ub-var} and \eqref{eq:ub-bias} are 
\begin{align}
    \var{\hat{f}(\mathbf{y})} &= O\left(\frac{1}{nh(\mathbf{y})^d}\right), \label{eq:mb-var} \\
    \left(\E \hat{f}(\mathbf{y}) - f(\mathbf{y})\right)^2 &= O\left(h(\mathbf{y})^{2p}\right). \label{eq:mb-bias}
\end{align}
Asymptotically, in the general case the optimal MISE is $O\left(n^{-2p/(2p+d)}\right)$. However, the constant factor in \eqref{eq:mb-bias} is actually a function with multiple terms depending on $f, K$, and $A$. This suggests that an intelligent choice of $A$ might lead to a cancellation of terms that reduces the asymptotic order of the bias. An estimator with such an $A$ is called an optimal multivariate balloon estimator. 

Restricting ourselves to the $p=2$ case, the authors show that the bias is 
\begin{align}
    \abs{\E \hat{f}(\mathbf{y}) - f(\mathbf{y})} \sim \frac{h(\mathbf{y})^2}{2} \text{tr} \left(A(\mathbf{y})^T \nabla^2f(\mathbf{y}) A(\mathbf{y})\right), 
\end{align}
where $\nabla^2f(\mathbf{y})$ is the Hessian of $f$ evaluated at $\mathbf{y}$. The authors consider several cases depending on the definiteness of the Hessian.

If $\nabla^2f(\mathbf{y})$ is positive or negative definite, then there is no way to improve the asymptotic MSE convergence rate beyond $n^{-4/(4+d)}$, but one can solve for the optimal matrix $A$ to be a solution of 
\begin{equation}
    AA^T = \abs{\nabla^2 f(\mathbf{y})}^{1/d} (\nabla^2 f(\mathbf{y}))^{-1}. 
\end{equation}
If $\nabla^2f(\mathbf{y})$ is saddle-shaped, then $A$ can actually be chosen so that the bias is asymptotically $o(h^2)$. The different signs of curvature of $f$ depending on the direction allow one to find an $A$ such that these contributions of these to the bias cancel each other out. If $\nabla^2f(\mathbf{y})$ is positive or negative semidefinite, with some zero eigenvalues, then the Hessian here is rank-deficient, and the task of finding $A$ is the same as in the definite Hessian case as if the dimension $d$ were lower. Thus, the only contributions to the asymptotic MISE of optimal multivariate balloon estimators are in regions where the Hessian of the density is definite. 

The authors report numerical computations of the theoretical factor between the asymptotic MISEs of the optimal balloon estimator and the optimal fixed estimator in the case where $f$ is the standard multinormal density. This factor is called the efficiency; an efficiency less than one means that the balloon estimator is performing better. Here, $f$ has definite Hessian only in the unit ball. The efficiency is show to decrease substantially as $d$ increases. For instance, at $d=6$, the efficiency is $<.06$. This suggests that optimal balloon estimators are promising when the dimension is large. 

In contrast to the optimal balloon estimator, but remaining in the $p=2$ case, the authors also present a multivariate analogue of the $k$th-nearest neighbor estimator called the Mahalanobis estimator. This has the form
\begin{equation}
    \hat{f}(\mathbf{y}) = \frac{1}{n \abs{H_k}} \sum_{i=1}^n K\left(H_k^{-1}(\mathbf{X}_i-\mathbf{y})\right), 
\end{equation}
where $H_k = h_kA$ for fixed $A$ and 
\begin{equation}
    (\mathbf{x}-\mathbf{y})(H_kH_k^T)^{-1}(\mathbf{x}-\mathbf{y}) \leq 1
\end{equation}
is the smallest ellipsoid centered at $y$ that contains $k$ data. Replacing $h_k$ with its limit for large $k$ and small $k/n$ and computing the asymptotic MISE shows that the optimal $k$ is $\asymp n^{4/(d+4)}$ and the optimal, which produces an MISE that is asymptotically $O(n^{-4/(d+4)})$. Again, the authors numerically compute the theoretical efficiency for several values of $d$ when the data is sampled from a multivariate standard normal distribution. For $d \leq 4$, the efficiency\footnote{Here, we keep the convention that an efficiency less than 1 corresponds with the balloon estimator performing better. This is not done in \cite{vkde}.} is greater than 1, but the efficiency is less than 1 for all other $d$, maximizing at 0.6472 when $d=15$.

\section{Numerical experiment}

Finally, we perform a numerical experiment comparing balloon and sample smoothing estimators with a fixed-width kernel density estimator. We first generate $N$ samples from a standard $d$-dimensional multivariate Gaussian distribution.

Our control estimator is a fixed-bandwidth estimator using a Gaussian kernel. Specifically, we use
\begin{equation}
    K_\textup{fixed-gaussian}(\mathbf{x}) = (2\pi)^{-d/2}\exp\left(-\frac{\mathbf{x}^T\mathbf{x}}{2}\right)
\end{equation}.

We choose $h \propto n^{-1/(d+4)}$, as this choice leads to an optimal MISE \cite{jnw}. Then, our fixed-width estimate is found by equation \eqref{eq:fixed-width-kde} with $K_\textup{fixed-gaussian}$ as our kernel.

For our balloon and sample smoothing estimator, we use a uniform density on the unit $d$-sphere, specifically,
\begin{equation}
    K_{\textup{variable-width}}(\mathbf{x}) = \frac{\Gamma(d/2+1)}{\pi^{d/2}}\mathbbm{1}_{\vert\mathbf{x}\vert\leq1}(\mathbf{x})
\end{equation}

For simplicity, we choose $k \propto \sqrt{N}$ for the k-nearest neighbor component. Note that this still respects the asymptotic limit $k/n \rightarrow 0$ and $k\rightarrow \infty$. Our balloon estimate is found using \eqref{eq:balloon-multi} and our sample smoothing estimate using \eqref{eq:ss1}. Figure \ref{fig:1d-visual} shows our density estimates in the one-dimensional case. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/1D_visual.png}
    \caption{One-dimensional density estimates using fixed-width, balloon, and sample smoothing estimates.}
    \label{fig:1d-visual}
\end{figure}

Finally, we need to compute the MISE to compare our estimators. In higher dimensions, this becomes difficult if we use a direct numerical integration. We estimate the MISE using a Monte Carlo integration method. We first choose a domain $\Omega$ that includes most of the true density. For our application, we choose a hypercube with length $L$, where we set $L=5$.

Then, we take $M$ sample points, $\mathbf{y}_i$ over that domain and estimate the MISE by 

\begin{equation}\label{eq:monte-carlo-MISE}
    \textup{MISE} \approx \frac{\textup{Vol}(\Omega)}{M}\sum_{i=1}^M \left(\hat{f}(\mathbf{y}_i) - f(\mathbf{y})\right)^2
\end{equation}

Figure \ref{fig:2d-visual} shows the squared error for each estimate in the two-dimensional case.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/2D_visual.png}
    \caption{Squared error for fixed-width, balloon, and sample smoothing estimates in two dimensions. The estimates were constructed from 12,800 samples from a standard bivariate normal distribution. Note the gaps in the plots, due to the Monte Carlo method we use for estimating the MISE.}
    \label{fig:2d-visual}
\end{figure}

Figure \ref{fig:MISE} shows the MISE for dimensions $d=1$ through $d=6$, along with an $N^{-4/(d+4)}$ line to show the minimum error asymptotic line. These numerical results support the conclusion that variable kernel density estimation only starts to improve over fixed-width KDE for higher dimensional data. In Figures \ref{fig:MISE-1}, the fixed width Gaussian has a smaller MISE for all sample sizes compared to both the balloon estimate and the sample smoothing estimate. However, for $d\geq3$, we can see in Figures \ref{fig:MISE-3}-\ref{fig:MISE-6} that the variable width kernel density estimates are a marked improvement over the fixed-width estimate.

\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.7\linewidth]{images/MISE_d=1.png}
  \caption{d=1}
  \label{fig:MISE-1}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.7\linewidth]{images/MISE_d=2.png}
  \caption{d=2}
  \label{fig:MISE-2}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.7\linewidth]{images/MISE_d=3.png}
  \caption{d=3}
  \label{fig:MISE-3}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.7\linewidth]{images/MISE_d=4.png}
  \caption{d=4}
  \label{fig:MISE-4}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.7\linewidth]{images/MISE_d=5.png}
  \caption{d=5}
  \label{fig:MISE-5}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.7\linewidth]{images/MISE_d=6.png}
  \caption{d=6}
  \label{fig:MISE-6}
\end{subfigure}
\caption{Mean Integrated Squared Error computed via Monte Carlo estimate for fixed-width Gaussian, balloon, and sample smoothing kernel density estimates. Each plot has the asymptotic $N^{-4/(d+4)}$ bound overlayed.}
\label{fig:MISE}
\end{figure}

\section{Data Availability}

All code used for the project can be found in the GitHub repository: \url{https://github.com/ndefilippis/Variable-Kernel-Density-Estimation-Math-Stats-Final-Project-}

\printbibliography

\end{document}